---
title: "Analysis of Hyperparameter Tuning Techniques"
output:
  html_document:
    df_print: paged
---

Set directory 

```{r, eval=FALSE, include=FALSE}
setwd("/Users/benchiddy/UCT/Gaussian Processes/Coding")
rm(list = ls())
```

Let's first load the MNIST data set and spilt into training and test sets. Note
that we also scale the data.

```{r, eval=FALSE}
require(dplyr)
require(laGP)
require(lhs)
require(keras)
require(RColorBrewer)

## Load MNIST data
mnist    <- dataset_mnist()
x_train  <- mnist$train$x
g_train  <- mnist$train$y
x_test   <- mnist$test$x
g_test   <- mnist$test$y

## Check dimensions
dim(x_train)
dim(x_test)

## Reshape data
x_train  <- array_reshape(x_train, c(nrow(x_train), 784))
x_test   <- array_reshape(x_test, c(nrow(x_test), 784))
y_train  <- to_categorical(g_train, 10)
y_test   <- to_categorical(g_test, 10)

## Scale data
x_train  <- x_train / 255
x_test   <- x_test / 255
```

Next lets create a Neural networ with enough capacity to overfit the problem
of classifying correct digits.

```{r, eval=FALSE}
modelnn <- function(alpha=0.5, lambda=0.2, epochs=30, vbose=0)
{
  #' Wrapper function for a (256, 128) Neural Network. Function allows for
  #' automatic tuning of two hyperparameters using different routines. Makes 
  #' use of the Keras library. Note that dplyr is used in building model.
  #' -----------------------------------------------------------------------
  #' @param aloha     object of type Numeric. Dropout rate on both layers.
  #' @param lambda    object of type Numeric. L2 regularization rate.
  #' @param epochs    object of type Numeric. Number of epochs during training.
  #' @param vbose     object of type Numeric. Either 1 or 0. Denotes whether
  #'                  user would like Keras training outputs to be displayed.
  #' -----------------------------------------------------------------------
  #' @return          object of type Numeric. Validation loss after the final
  #'                  epoch.
  #' -----------------------------------------------------------------------
  library(dplyr)
  library(keras)
  
  ## Create sequential Keras model
  modelnn <- keras_model_sequential() %>%
    layer_dense(
      units=256,
      activation="relu",
      input_shape=c(784)
    ) %>%
    layer_dropout(alpha) %>%
    layer_dense(
      units=128,
      activation="relu"
    ) %>%
    layer_dropout(alpha) %>%
    layer_dense(
      regularizer_l2(lambda),
      units=10,
      activation="softmax"
    )
  
  ## Compile model
  modelnn %>%
    compile(
      loss="categorical_crossentropy",
      optimizer_rmsprop(),
      metrics=c("accuracy")
    )
  
  ## Fit model
  history <- modelnn %>%
    fit(
      x_train,
      y_train,
      epochs=epochs,
      batch_size=128,
      validation_split=0.2,
      verbose=vbose
    )
  
  ## Return validation loss at the end of training
  ret <- history$metrics$val_loss[epochs]
  return(ret)
}

## Start up Keras
modelnn(epochs=1)
```

# Tuning algorithms

Random Search

```{r, eval=FALSE}
random_search <- function(f, n=25, num_par=2, hmin=0, hmax=1, names)
{
  #' Proposes n sets of hp vectors within stated bounds using Latin Hypercube 
  #' Sampling. Will then run f and return a data frame containing all attempted 
  #' hp vectors and their associated output.
  #' -----------------------------------------------------------------------
  #' @param f         object of type Function. Function to evaluate hps over.
  #' @param n         object of type Numeric. Amount of hp vectors to try.
  #' @param num_par   object of type Numeric. Number of hp values needed.
  #' @param hmin      object of type Numeric. Lower bound for hp values.
  #' @param hmax      object of type Numeric. Upper bound for hp values.
  #' @param names     object of type Character. Vector of hp names.
  #' -----------------------------------------------------------------------
  #' @return          object of type Data Frame. hps and their outputs on f.
  #' -----------------------------------------------------------------------

  ## Initialize important objects
  y   <- c()
  X   <- randomLHS(n, num_par) # Sample random values using an LHS scheme
  
  ## 'Optimization' loop
  for(i in 1:n) {
    hps  <- X[i,]
    ynew     <- do.call(f, as.list(hps)) # Run f at specified hp setting
    y        <- c(y, ynew)
  }
  
  ## Return hps and their outputs on f
  D             <- data.frame(cbind(X,y))
  colnames(D)   <- c(names, "y")
  return(D)
}
```

Grid search

```{r, eval=FALSE}
grid_search <- function(f, points, names)
{
  #' Creates a grid containing all possible combinations of proposed hp values.
  #' Will then run f at each hp combination and return a data frame containing
  #' all combinations and their associated output
  #' -----------------------------------------------------------------------
  #' @param f         object of type Function. Function to evaluate hps over.
  #' @param points    object of type List. Proposed values for each hp. 
  #'                  Note these must be of equal length.
  #' @param names     object of type Character. Vector of hp names.
  #' -----------------------------------------------------------------------
  #' @return          object of type Data Frame. hps and their outputs on f
  #' -----------------------------------------------------------------------

  ## Initialize important objects
  extent   <- length(points[[1]]) # number of proposed values for each hp
  num_par  <- length(points)      # number of hps being tested
  x        <- matrix(nrow = num_par, ncol = extent)
  y        <- c() 
  
  ## Check each vector of proposed hp values has equal length. Exit if not.
  for (i in 1:num_par) 
  {
    if(length(points[[i]]) != extent)
      return("Points should be equal for all hyperparameters")
  }
  
  ## Create grid of all possible combinations for proposed hp values.
  for (i in 1:num_par) 
  {
    x[i,] <- points[[i]]
  }
  grid             <- as.matrix(expand.grid(x[1,], x[2,]))
  colnames(grid)   <- c()
  
  ## 'Optimization' loop
  for(i in 1:nrow(grid)) 
  {
    hps       <- grid[i,]
    ynew      <- do.call(f, as.list(hps)) # Run f at specified hp setting
    y         <- c(y, ynew)
  }
  
  ## Return hps and their outputs on f
  D           <- data.frame(cbind(grid,y))
  colnames(D) <- c(names, "y")
  return(D)
}
```

Bayesian Optimization main wrapper

```{r, eval=FALSE}
bayes_opt <- function(f, ninit, num_par, hmin, hmax, 
                      end, names, type="pi", plt=FALSE)
{
  #' Uses Surrogate Assisted (Bayesian) Optimization to find an 'optimal' 
  #' set of hp values within a specified budget. The Gaussian Process is 
  #' used as a surrogate or prior on the function which maps hp values to 
  #' their outputs. An acquisition strategy is then used to select the 
  #' next hp setting to attempt. Currently only 'Predicted Improvement' is 
  #' implemented with 'Expected Improvement' and 'Upper Confidence Bound'
  #' in development. Function will return both a data frame containing 
  #' hps and their outputs as well as a list containing the predictive
  #' surface at each time point for animating the algorithm.
  #' -----------------------------------------------------------------------
  #' @param f         object of type Function. Function to evaluate hps over.
  #' @param ninit     object of type Numeric. Amount of initial points to 
  #'                  sample (using LHS) before starting the BO algorithm. 
  #' @param num_par   object of type Numeric. Number of hp values needed.
  #' @param hmin      object of type Numeric. Lower bound for hp values.
  #' @param hmax      object of type Numeric. Upper bound for hp values.
  #' @param end       object of type Numeric. Maximum number a steps.        
  #' @param names     object of type Character. Vector of hp names.
  #' @param type      object of type Character. Type of acquisition strategy.
  #'                  pi  = Predicted Improvement
  #'                  ei  = Expected Improvement (WIP)
  #'                  ucb = Upper Confidence Bound (WIP)
  #' @param plt       object of type Character. Specify whether or not 
  #'                  plotting should be done during runtime.
  #' -----------------------------------------------------------------------
  #' @return          object of type List. Data frame of hps and their outputs 
  #'                  on f as well as a list of surface states collected 
  #'                  during runtime.
  #' -----------------------------------------------------------------------

  ## Initialize important objects
  states <- list()
  step   <- 1
  X      <- randomLHS(ninit, num_par) # Sample initial values using LHS
  y      <- c() 
  cols   <- c("blue", "purple", "white", "orange", "red")
  colour <- colorRampPalette(cols)(100)
  
  ## Run f at initial locations
  for (i in 1:ninit) 
  {
    hparams  <- X[i,] # Run f at specified hp setting
    y[i]     <- do.call(f, as.list(hparams))
  }
  
  ## Create a grid over hp space to create predictive surface
  xx     <- seq(0, 1, length=80)
  XX     <- expand.grid(xx, xx)
  
  ## Fit GP using initially sampled locations (makes use of laGP package)
  da      <- darg(list(mle=TRUE, max=hmax), randomLHS(1000, num_par)) 
  gpi     <- newGPsep(X, y, d=da$start, g=1e-6, dK=TRUE) 
  mleGPsep(gpi, param="d", tmin=da$min, tmax=da$max, ab=da$ab)
  
  ## Create predictive surface for hp space and save at current timestep.
  states[[step]]  <- predGPsep(gpi, XX)$mean
  step            <- step + 1
  
  ## Plots current predictive surface for hp space
  if (plt) 
  {
    Z <- matrix(states[[step-1]], ncol=length(xx))
    image(xx, xx, Z, col=colour, ylab="", xlab="")
    title(xlab = expression(lambda), ylab = expression(alpha), adj = 0.5)
    points(X, pch = 4)
  }
  
  ## Start BO algorithm using Predicted improvement strategy.
  if (type == "pi") 
    pi <- predicted_improvement(f, gpi, X, y, xx=xx, XX=XX, 
                                states=states, end=end, da=da, 
                                plt=plt, ninit=ninit)
  
  ## Start BO algorithm using Expected Improvement strategy.
  if (type == "ei") 
    return("Not implemented yet.\n")
  
  ## Start BO algorithm using Upper Confidence Bound strategy.
  if (type == "ucb") 
    return("Not implemented yet.\n")
  
  ## Clean up and return results of BO algorithm
  D              <- data.frame(pi$data)
  colnames(D)    <- c(names, "y") # Rename hp columns
  bo_results     <- list(data = D, states = pi$states)
  return(bo_results)
}
```

# Acquisition strategies

Predicted improvement

```{r, eval=FALSE}
predicted_value <- function(x, gpi) 
{
  #' Helper function for Predicted Improvement acquisition strategy. Returns
  #' predicted value at a given coordinate within the surrogate hp space.
  #' -----------------------------------------------------------------------
  #' @param x         object of type Numeric. Vector of hp cooridnates.
  #' @param gpi       object of type laGP. Fitted Gaussian Process.
  #' -----------------------------------------------------------------------
  #' @return          object of type Numeric. Predicted f value in hp space.
  #' -----------------------------------------------------------------------

  return(predGPsep(gpi, matrix(x, nrow=1), lite=TRUE)$mean)
}

predicted_improvement <- function(f, gpi, X, y, xx, XX, states, 
                                  step=2, end, da, plt=FALSE, ninit) 
{
  #' Runs Bayesian Optimization using the Predicted Improvement acquisition 
  #' strategy. Not intended to be run by user, access provided through 
  #' bayes_opt function. Returns list of BO algorithm results as well as the 
  #' predictive surface at each timestep.
  #' -----------------------------------------------------------------------
  #' @param f         object of type Function. Function to evaluate hps over.
  #' @param gpi       object of type laGP. Fitted Gaussian Process. 
  #' @param X         object of type Matrix Array. Initial hp values tested.
  #' @param y         object of type Numeric. Values for f obtained at X.
  #' @param xx        object of type Numeric. Values to create hp grid.
  #' @param XX        object of type Data Frame. Prediction grid for plotting.
  #' @param states    object of type List. Predictive surfaces at each timestep.
  #' @param ninit     object of type Numeric. Number of initial points.
  #' @param step      object of type Numeric. Current timestep.
  #' @param end       object of type Numeric. Maximum number of iterations.
  #' @param da        object of type List. laGP specific object.
  #' @param plt       object of type Character. Specify whether or not 
  #'                  plotting should be done during runtime.
  #' -----------------------------------------------------------------------
  #' @return          object of type List. Data frame of hps and their outputs 
  #'                  on f as well as a list of surface states collected 
  #'                  during runtime.
  #' -----------------------------------------------------------------------
  
  ## Initialize important objects
  cols   <- c("blue", "purple", "white", "orange", "red")
  colour <- colorRampPalette(cols)(100)
  
  ## Optimization loop
  for(i in (nrow(X)+1):end) {
    
    ## Using the current minimum as an initial value use L-BFGS-B to obtain
    ## the predicted global minimum hp values based on the GP Surrogate.
    m         <- which.min(y)
    hparams   <- optim(X[m,], predicted_value, lower=0.001, upper=0.999, 
                     method="L-BFGS-B", gpi=gpi)$par
    ynew      <- do.call(f, as.list(hparams)) # Run f at proposed hp values.
    
    ## Update GP Surrogate based on new information
    updateGPsep(gpi, matrix(hparams, nrow=1), ynew) 
    mleGPsep(gpi, param="d", tmin=da$min, tmax=da$max, ab=da$ab) 
    
    ## Update dataset on hp space
    X            <- rbind(X, hparams)
    rownames(X)  <- NULL # Reset row names. Assists with plotting.
    y            <- c(y, ynew)
    
    ## Log predictive surface at current timestep.
    states[[step]]  <- predGPsep(gpi, XX)$mean
    step            <- step + 1 
    
    ## Plots current predictive surface for hp space
    if (plt) 
    {
      # Plot predictive surface
      Z <- matrix(states[[step-1]], ncol=length(xx))
      image(xx, xx, Z, col=colour, ylab="", xlab="")
      title(xlab = expression(lambda), ylab = expression(alpha), adj = 0.5)
      
      # Plot the initial points
      points(X[1:ninit,], pch=4)
      
      #' Plots the points selected through predicted improvement. 
      #' Ugly if-else is the only way I could fix a plotting bug.
      #' Will fix in the future.
      if ((ninit+1)==nrow(X)) {
        points(t(X[(ninit+1):nrow(X),]), pch=19, cex=0.7)
      } else {
        points(X[(ninit+1):nrow(X),], pch=19, cex=0.7)
      }
      
      # Plot lines showing where point was selected.
      abline(v=X[nrow(X),1], lty = 2)
      abline(h=X[nrow(X),2], lty = 2)
    }
  }
  
  ## Clean and return results of BO algorithm
  deleteGPsep(gpi) 
  pi <- list(data = as.matrix(cbind(X,y)), states = states)
  return(pi)
}
```

Bake-off between tuning techniques.

```{r, eval=FALSE}
data <- data.frame()
runs <- 2

for (i in 1:runs) {
  
  ## Initialize MC run
  nms <- c("alpha", "lambda")
  cat("--------------------------------------------------------\n")
  tic_main <- Sys.time()
  cat("Starting MC simulation", i, "of", runs, ":\n")
  
  ## Start random search
  cat("Random search starting ...\n")
  tic <- Sys.time()
  x <- random_search(f=modelnn, n=25, hmin=0, hmax=0.999, 
                     num_par = 2, names = nms)
  toc <- round(difftime(Sys.time(), tic, units = "mins"), 2)
  cat("Random search completed in", toc, "minutes.\n")
  
  ## Log random search results
  data <- rbind(data, data.frame(
    alpha   = x$alpha,
    lambda  = x$lambda,
    y       = x$y,
    type    = "random",
    run     = i
  ))
  
  ## Start grid search
  cat("Grid search starting ...\n")
  tic <- Sys.time()
  p <- list(c(0.01, 0.25, 0.5, 0.75, 0.9), c(0.01, 0.25, 0.5, 0.75, 0.9))
  x <- grid_search(f=modelnn, points=p, names=nms)
  toc <- round(difftime(Sys.time(), tic, units = "mins"), 2)
  cat("Grid search completed in", toc, "minutes.\n")
  
  ## Log grid search results
  data <- rbind(data, data.frame(
    alpha   = x$alpha,
    lambda  = x$lambda,
    y       = x$y,
    type    = "grid",
    run     = i
  ))
  
  ## Start bayesian optimisation
  cat("Bayesian optimisation starting ...\n")
  tic <- Sys.time()
  x <- bayes_opt(f=modelnn, ninit = 12, end = 25, num_par = 2, 
                  hmin = 0, hmax = 1, names = nms, type = "pi")
  toc <- round(difftime(Sys.time(), tic, units = "mins"), 2)
  cat("Bayesian optimisation completed in", toc, "minutes.\n")
  
  ## Log bayesian optimisation
  data <- rbind(data, data.frame(
    alpha   = x$data$alpha,
    lambda  = x$data$lambda,
    y       = x$data$y,
    type    = "bayesian",
    run     = i
  ))
  
  ## Write state information to csv to reduce memory load.
  for ( j in 1:length(x$states))
  {
    
    # Create the directory if it doesn't exist
    dir_name <- paste0("data/mc_run", i)
    if (!dir.exists(dir_name)) 
    {
      dir.create(dir_name, recursive = TRUE)
    }
    
    # Write the data frame to CSV
    file_name <- paste0(dir_name, "/", "state", j, ".csv")
    preds <- data.frame(x$states[[j]])
    colnames(preds) <- "pred"
    write.csv(preds, file = file_name)
  }
  
  ## End MC run
  toc_main <- round(difftime(Sys.time(), tic_main, units = "mins"), 2)
  time_left <- toc_main*(runs-i)
  cat("--------------------------------------------------------\n")
  cat("MC simulation", i, "successfully completed in", 
      toc_main, "minutes.\n")
  cat("Predicted time left:", time_left, "minutes or", 
      round(time_left/60, 2), "hours.\n")
  
  ## If last run close off results. Just for aesthetics.
  if (i == runs)
  {
    cat("--------------------------------------------------------\n")
  }
}

write.csv(data, "data/most_recent_mc.csv")
```

Plotting results. Very messy, planning to clean.

```{r, eval=FALSE}
## Best objective function value
bov <- function(y, end=length(y)) 
{
  prog <- rep(min(y), end)
  prog[1:min(end, length(y))] <- y[1:min(end, length(y))] 
  for(i in 2:end)
    if(is.na(prog[i]) || prog[i] > prog[i-1]) prog[i] <- prog[i-1] 
  return(prog)
}


## Making animations for presentation
data <- read.csv("data/most_recent_mc.csv", header = TRUE)[,2:5]
bmax <- max(data$y); bmin <- min(data$y)

# Random search
dt <- subset(data, type == "random")
b <- bov(dt$y)
par(mfrow=c(1,2))
for (i in 1:length(b)) {
  plot(b[1:i], type='s', 
       xlim=c(1,length(b)),
       ylim=c(min(b), max(b)),
       xlab="Function evaluations", 
       ylab="Best objective function value")
  plot(1, type='n', xlim=c(0,1), ylim=c(0,1), xlab="", ylab="")
  points(matrix(c(dt[1:i,"alpha"], dt[1:i,"lambda"]), 
                ncol=2), pch=19, cex=0.5)
  abline(v=dt[i,1], lty = 2); abline(h=dt[i,2], lty = 2)
  title(xlab = expression(lambda), ylab = expression(alpha), adj = 0.5)
  Sys.sleep(1)
}

# Grid search
dt <- subset(data, type == "grid")
b <- bov(dt$y)
par(mfrow=c(1,2))
for (i in 1:length(b)) {
  plot(b[1:i], type='s', 
       xlim=c(1,length(b)),
       ylim=c(min(b), max(b)),
       xlab="Function evaluations", 
       ylab="Best objective function value")
  plot(1, type='n', xlim=c(0,1), ylim=c(0,1), xlab="", ylab="")
  points(matrix(c(dt[1:i,"alpha"], dt[1:i,"lambda"]), 
                ncol=2), pch=19, cex=0.5)
  abline(v=dt[i,1], lty = 2); abline(h=dt[i,2], lty = 2)
  title(xlab = expression(lambda), ylab = expression(alpha), adj = 0.5)
  Sys.sleep(1)
}

# Bayesian
dt <- subset(data, type == "bayesian")
st <- read.csv("data/mc_run1/state1.csv", header = TRUE)[,2]
b <- bov(dt$y)
xx     <- seq(0, 1, length=80)
colour <- colorRampPalette(c("blue", "purple", "white", "orange", "red"))(100)
par(mfrow=c(1,2))
init <- 12

## Initialze
plot(b[1:1], type='s', 
       xlim=c(1,length(b)),
       ylim=c(min(b), max(b)),
       xlab="Function evaluations", 
       ylab="Best objective function value")
legend("topright", legend = "Initial \nSample", 
         lty = 2, bty = 'n', cex=0.7)
abline(v=init, lty=2)
plot(1, type='n', xlim=c(0,1), ylim=c(0,1), xlab="", ylab="")
title(xlab = expression(lambda), ylab = expression(alpha), adj = 0.5)
for (i in 1:length(b)) {
  
  plot(b[1:i], type='s', 
       xlim=c(1,length(b)),
       ylim=c(min(b), max(b)),
       xlab="Function evaluations", 
       ylab="Best objective function value")
  legend("topright", legend = "Initial \nSample", 
         lty = 2, bty = 'n', cex=0.7)
  abline(v=init, lty=2)
  
  if (i <= init) {
    
    plot(1, type='n', xlim=c(0,1), ylim=c(0,1), xlab="", ylab="")
    points(matrix(c(dt[1:i,"alpha"], dt[1:i,"lambda"]),
                  ncol=2), pch=4, cex=0.5)
    abline(v=dt[i,1], lty = 2); abline(h=dt[i,2], lty = 2)
    title(xlab = expression(lambda), ylab = expression(alpha), adj = 0.5)
    
  } else {
    
    file <- paste0("data/mc_run1/state", i-init, ".csv")
    st <- read.csv(file, header = TRUE)[,2]
  
    Z <- matrix(st, ncol=length(xx))
    image(xx, xx, Z, col=colour, ylab="", xlab="")
    points(matrix(c(dt[1:init,"alpha"], dt[1:init,"lambda"]),
                  ncol=2), pch=4, cex=0.7)
    points(matrix(c(dt[(init+1):i,"alpha"], dt[(init+1):i,"lambda"]),
                  ncol=2), pch=19, cex=0.5)
    abline(v=dt[i,1], lty = 2); abline(h=dt[i,2], lty = 2)
    title(xlab = expression(lambda), ylab = expression(alpha), adj = 0.5)
  }
  
  Sys.sleep(1)
}
```